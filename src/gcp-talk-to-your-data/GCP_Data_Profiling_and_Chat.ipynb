{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCP Data Profiling & Talk to Your Data\n",
    "\n",
    "This notebook:\n",
    "1. **Auto-detects** your GCP data sources (GCS buckets, BigQuery datasets/tables) using Workbench credentials.\n",
    "2. **Generates an automated data profiling report** (ydata-profiling) for the source you pick.\n",
    "3. **Talk to your data**: you provide an LLM API key when you run that section; then ask any question about the loaded data.\n",
    "\n",
    "Run cells in order. First run the setup and discovery cells, then pick a data source and load it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Install packages if not already present (run once)\n",
    "try:\n",
    "    import google.cloud.storage\n",
    "    import ydata_profiling\n",
    "    import openai\n",
    "except ImportError:\n",
    "    req = \"/workspace/requirements.txt\" if os.path.exists(\"/workspace/requirements.txt\") else \"/home/jovyan/requirements.txt\"\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-r\", req])\n",
    "    print(\"Dependencies installed. Re-run this cell if needed.\")\n",
    "else:\n",
    "    print(\"Dependencies OK.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Authenticate with GCP & discover data sources\n",
    "\n",
    "Uses **Application Default Credentials** (Workbench provides these). No API key needed for GCP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gcp_tools as gt\n",
    "\n",
    "# Get default GCP project (from Workbench / ADC)\n",
    "GCP_PROJECT = gt.get_default_project()\n",
    "if GCP_PROJECT:\n",
    "    print(f\"Default GCP project: {GCP_PROJECT}\")\n",
    "else:\n",
    "    print(\"Could not detect default project. Set GCP_PROJECT manually below.\")\n",
    "    GCP_PROJECT = \"\"  # e.g. \"your-project-id\"\n",
    "\n",
    "# Discover GCS buckets\n",
    "buckets = gt.list_gcs_buckets(GCP_PROJECT)\n",
    "print(f\"GCS buckets found: {len(buckets)}\")\n",
    "if buckets:\n",
    "    for b in buckets[:20]:\n",
    "        print(f\"  - {b}\")\n",
    "    if len(buckets) > 20:\n",
    "        print(f\"  ... and {len(buckets)-20} more\")\n",
    "\n",
    "# Discover BigQuery datasets (if project set)\n",
    "bq_datasets = []\n",
    "if GCP_PROJECT:\n",
    "    try:\n",
    "        bq_datasets = gt.list_bigquery_datasets(GCP_PROJECT)\n",
    "        print(f\"\\nBigQuery datasets in {GCP_PROJECT}: {len(bq_datasets)}\")\n",
    "        for d in bq_datasets[:15]:\n",
    "            print(f\"  - {d}\")\n",
    "        if len(bq_datasets) > 15:\n",
    "            print(f\"  ... and {len(bq_datasets)-15} more\")\n",
    "    except Exception as e:\n",
    "        print(f\"BigQuery list error: {e}\")\n",
    "\n",
    "# Store for later cells\n",
    "BUCKETS = buckets\n",
    "BQ_DATASETS = bq_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pick a data source and load it\n",
    "\n",
    "Set the variables below to your chosen source, then run the load cell.\n",
    "\n",
    "- **GCS**: set `SOURCE_TYPE = 'GCS'`, `GCS_BUCKET`, `GCS_PATH`, and `GCS_FORMAT` (e.g. `'csv'`, `'parquet'`).\n",
    "- **BigQuery**: set `SOURCE_TYPE = 'BigQuery'`, `BQ_PROJECT`, `BQ_DATASET`, `BQ_TABLE`. Optionally set `BQ_LIMIT` (default 100000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Set your data source here ---\n",
    "SOURCE_TYPE = \"GCS\"  # or \"BigQuery\"\n",
    "\n",
    "# For GCS:\n",
    "GCS_BUCKET = BUCKETS[0] if BUCKETS else \"\"  # or type bucket name\n",
    "GCS_PATH = \"\"  # e.g. \"path/to/file.csv\"\n",
    "GCS_FORMAT = \"csv\"  # csv, parquet, json\n",
    "\n",
    "# For BigQuery:\n",
    "BQ_PROJECT = GCP_PROJECT or \"\"\n",
    "BQ_DATASET = BQ_DATASETS[0] if BQ_DATASETS else \"\"\n",
    "BQ_TABLE = \"\"  # e.g. \"mytable\"\n",
    "BQ_LIMIT = 100000\n",
    "\n",
    "# Optional: list files in a bucket to choose one\n",
    "if SOURCE_TYPE == \"GCS\" and GCS_BUCKET and not GCS_PATH:\n",
    "    files = gt.list_gcs_blobs(GCS_BUCKET, max_results=100)\n",
    "    print(f\"Sample files in {GCS_BUCKET}: \")\n",
    "    for f in files[:30]:\n",
    "        print(f\"  {f}\")\n",
    "    if len(files) > 30:\n",
    "        print(f\"  ... and {len(files)-30} more\")\n",
    "    print(\"\\nSet GCS_PATH to one of the paths above and re-run the next cell.\")\n",
    "\n",
    "# Optional: list tables in a dataset to choose one\n",
    "if SOURCE_TYPE == \"BigQuery\" and BQ_PROJECT and BQ_DATASET and not BQ_TABLE:\n",
    "    tables = gt.list_bigquery_tables(BQ_PROJECT, BQ_DATASET)\n",
    "    print(f\"Tables in {BQ_PROJECT}.{BQ_DATASET}: {tables}\")\n",
    "    print(\"Set BQ_TABLE and re-run the next cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into DataFrame\n",
    "if SOURCE_TYPE == \"GCS\" and GCS_BUCKET and GCS_PATH:\n",
    "    df = gt.load_from_gcs(GCS_BUCKET, GCS_PATH, GCS_FORMAT)\n",
    "    print(f\"Loaded from gs://{GCS_BUCKET}/{GCS_PATH}: {len(df):,} rows, {len(df.columns)} columns\")\n",
    "elif SOURCE_TYPE == \"BigQuery\" and BQ_PROJECT and BQ_DATASET and BQ_TABLE:\n",
    "    df = gt.load_from_bigquery(BQ_PROJECT, BQ_DATASET, BQ_TABLE, limit=BQ_LIMIT)\n",
    "    print(f\"Loaded from BigQuery {BQ_PROJECT}.{BQ_DATASET}.{BQ_TABLE}: {len(df):,} rows, {len(df.columns)} columns\")\n",
    "else:\n",
    "    df = None\n",
    "    print(\"Set SOURCE_TYPE and the required variables in the previous cell, then re-run.\")\n",
    "\n",
    "if df is not None:\n",
    "    display(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4a. Automated data profiling report\n",
    "\n",
    "Generates a **ydata-profiling** report: distributions, null %, min/max, and more for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is None:\n",
    "    print(\"Load a data source above first.\")\n",
    "else:\n",
    "    from ydata_profiling import ProfileReport\n",
    "\n",
    "    profile = ProfileReport(\n",
    "        df,\n",
    "        title=\"Data Profiling Report\",\n",
    "        explorative=True,\n",
    "        minimal=False,\n",
    "    )\n",
    "    profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: save report to HTML file\n",
    "if df is not None:\n",
    "    profile.to_file(\"/workspace/data_profile_report.html\")\n",
    "    print(\"Report saved to /workspace/data_profile_report.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4b (optional). Get company OpenAI key from Secret Manager\n",
    "\n",
    "If your company provides an OpenAI API key via **Google Cloud Secret Manager**, run the cell below first. Set:\n",
    "- **project_id**: GCP project where the secret lives. For VWB Sandbox use `wb-smart-cabbage-5940`. For prod, use your workspace project (or get it from `wb status`).\n",
    "- **team_alias**: Your team's secret prefix. The secret name is `{team_alias}openai-api-key`. Examples: `ml-platform-test-`, `compbio-`, `it-team-`, etc.\n",
    "\n",
    "The key is fetched and stored in `LLM_API_KEY` and `OPENAI_BASE_URL` for the \"Talk to your data\" cells below. If you skip this, you can paste your own key when prompted in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Company key from Secret Manager (run this if you use it) ---\n",
    "USE_SECRET_MANAGER = True  # Set to False to skip and paste your own key later\n",
    "\n",
    "# VWB Sandbox (typical for dev). For prod, use your workspace project id.\n",
    "SECRET_PROJECT_ID = \"wb-smart-cabbage-5940\"\n",
    "# Your team's alias; secret name = {team_alias}openai-api-key\n",
    "SECRET_TEAM_ALIAS = \"ml-platform-test-\"  # or compbio-, it-team-, participant-ops-, etc.\n",
    "\n",
    "LLM_API_KEY = None\n",
    "OPENAI_BASE_URL = \"https://us.api.openai.com/v1/\"  # Company endpoint\n",
    "\n",
    "if USE_SECRET_MANAGER and SECRET_PROJECT_ID and SECRET_TEAM_ALIAS:\n",
    "    try:\n",
    "        LLM_API_KEY = gt.get_openai_key_from_secret_manager(SECRET_PROJECT_ID, SECRET_TEAM_ALIAS)\n",
    "        print(\"Company OpenAI key loaded from Secret Manager.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Secret Manager error: {e}\")\n",
    "        print(\"You can still paste your own key in the Talk to your data cell below.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4c. Talk to your data\n",
    "\n",
    "If you ran the Secret Manager cell above and the key was loaded, it will be used automatically. Otherwise you will be prompted to **paste your LLM API key** (not stored). Then ask any question about the loaded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is None:\n",
    "    print(\"Load a data source above first.\")\n",
    "else:\n",
    "    from getpass import getpass\n",
    "\n",
    "    if not (LLM_API_KEY and str(LLM_API_KEY).strip()):\n",
    "        LLM_API_KEY = getpass(\"Paste your LLM API key (e.g. OpenAI): \")\n",
    "    openai_base = globals().get(\"OPENAI_BASE_URL\") or \"https://api.openai.com/v1\"\n",
    "    data_summary, schema_and_sample = gt.data_summary_for_llm(df)\n",
    "    question = input(\"Ask a question about the data: \")\n",
    "    answer = gt.talk_to_data(\n",
    "        api_key=LLM_API_KEY,\n",
    "        data_summary=data_summary,\n",
    "        schema_and_sample=schema_and_sample,\n",
    "        question=question,\n",
    "        model=\"gpt-4o\",  # or gpt-4o-mini\n",
    "        base_url=openai_base,\n",
    "    )\n",
    "    print(\"\\n--- Answer ---\\n\")\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ask more questions (re-use same key)\n",
    "\n",
    "Run the cell below to ask follow-up questions. Paste your API key again when prompted (or leave blank to skip)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell multiple times to ask more questions about the same df\n",
    "if df is None:\n",
    "    print(\"Load a data source above first.\")\n",
    "else:\n",
    "    from getpass import getpass\n",
    "\n",
    "    key = LLM_API_KEY if globals().get(\"LLM_API_KEY\") else getpass(\"LLM API key (or press Enter to skip): \")\n",
    "    if key:\n",
    "        data_summary, schema_and_sample = gt.data_summary_for_llm(df)\n",
    "        q = input(\"Question: \")\n",
    "        if q:\n",
    "            base = globals().get(\"OPENAI_BASE_URL\") or \"https://us.api.openai.com/v1/\"\n",
    "            print(gt.talk_to_data(key, data_summary, schema_and_sample, q, base_url=base))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
